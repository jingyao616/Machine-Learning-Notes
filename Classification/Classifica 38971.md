# Classification

# [Logistics regression](https://www.youtube.com/watch?v=yIYKR4sgzI8)

- It relies on a specific model relating predictor variables with an outcome (target) variable.
- Due to the presence of the logit lnk function, parameter estimation is done using **maximum likelihood estimates**
- æŠŠclassificationé—®é¢˜ï¼Œç”¨logè½¬æ¢æˆæ•°å­—ï¼Œå†ç”¨probabilityæ¥åšã€‚

åˆ¤æ–­true/falseç§ç±»

![13E61232-5B2C-46BE-BC15-FD22339784CF.png](Classifica%2038971/13E61232-5B2C-46BE-BC15-FD22339784CF.png)

<aside>
ğŸ’¡ ä¸ä»…èƒ½åšclassificationé¢„æµ‹ï¼Œè¿˜èƒ½å¾—å‡ºpossibility on eachã€‚

</aside>

## [Decision tree](https://www.youtube.com/watch?v=7VeUPuFGJHk)

Root, Node, leaf

æ€ä¹ˆåˆ¤æ–­è°æ¥åšç¬¬ä¸€ä¸ªnodeï¼Ÿâ€”> Measures of **Node Impurity** 

- Gini
    - Gini impurity é€‰æœ€å°çš„
    - Yes No question / numerical (å…ˆrankç„¶åç®—ä¸¤æ•°å¹³å‡æ•°çš„Giniï¼‰
    - ranking / multiple choices (æœ€åä¸€ç§å¯èƒ½æ€§é€‰é¡¹ä¸ç”¨ç®—ï¼‰
    
    ![Screen Shot 2022-03-06 at 11.33.58 PM.png](Classifica%2038971/Screen_Shot_2022-03-06_at_11.33.58_PM.png)
    
- [Entropy](https://towardsdatascience.com/entropy-how-decision-trees-make-decisions-2946b9c18c8)
    - Entropy is a measure of disorder or uncertainty and the goal of machine learning models and Data Scientists in general is to reduce uncertainty.
        
        ![3C03349B-C758-4AE2-A912-99F8C2B99F2A.jpeg](Classifica%2038971/3C03349B-C758-4AE2-A912-99F8C2B99F2A.jpeg)
        
- Noteï¼šä¸¤ç§æ”¾éƒ½è¦è®°å¾—weighted averageç®—åˆ†æ•°

<aside>
ğŸ’¡ Regressions offer a different approach to prediction compared to decision trees. **Regressions**, as **parametric models**. **assume a specific association structure between inputs and target**. By contrast, **trees**, as **predictive algorithms**, **do not assume any association structure**; they simply seek to isolate concentrations of cases with like-valued target measurements.

</aside>

## [Random forest](https://www.youtube.com/watch?v=J4Wdy0Wc_xQ&t=354s)

ç”¨å•ä¸ªdecision treeå¯¹äºæ–°çš„ä¸åŒç§ç±»ä¸ä¸€å®šå‡†ç¡®ï¼ˆinaccurateï¼‰ï¼Œæ‰€ä»¥å¤šç”¨sampleåšå¥½å¤šrandom treeã€‚

æ€ä¹ˆé€‰ç”¨å‡ ä¸ªvariableï¼Ÿ-è¯•ï¼

ç”¨ä»€ä¹ˆéªŒè¯ï¼Ÿ-æ²¡è¢«pickçš„sampleï¼ˆä¸€èˆ¬1/3çš„sampleï¼‰

å¦‚æœæœ‰missing dataï¼Ÿ-åˆ†åœ¨sampleé‡Œmissingè¿˜æ˜¯è¦predictçš„dataé‡Œmissingã€‚ä½†åŸºæœ¬åŸç†éƒ½æ˜¯ç”¨ç»“æœåæ¨ã€‚

![6E0D838E-BF0D-4969-BAE7-6EC9F60A6401.png](Classifica%2038971/6E0D838E-BF0D-4969-BAE7-6EC9F60A6401.png)

## [Boosting tree](https://www.youtube.com/watch?v=3CC4N4z3GJc)

- When Gradient Boost is used to **Predict** a **continuous** value, like **Weight**, we say that weâ€™re using Gradient Boost for **Regression**(è·Ÿliner Regressionä¸ä¸€æ ·ï¼Œæ³¨æ„åŒºåˆ«ï¼‰.
    
    ![Screen Shot 2022-03-08 at 2.20.44 PM.png](Classifica%2038971/Screen_Shot_2022-03-08_at_2.20.44_PM.png)
    
- è·ŸadaBoost ç›¸æ¯”ï¼ŒGradient Boostæ˜¯**åç€æ¨**
    1. average value of the variable we want to **predict**
    2. å»ºä¸€ä¸ªtree based on the residuals 
    3. scale the treeâ€™s contribution to the final Prediction with a Learning Rate(0-1)
    4. Add another tree based on the residuals 
    5. ç†è®ºå°±æ˜¯æ¯æ¬¡one small step to the right directionï¼ŒåŠ åœ¨ä¸€èµ·å°±æ˜¯éå¸¸accurateäº†
        
        ![Screen Shot 2022-03-08 at 11.16.51 PM.png](Classifica%2038971/Screen_Shot_2022-03-08_at_11.16.51_PM.png)
        
- When Gradient Boost is used for **Classification**, it has a lot in common with Logistic Regression.
- ä¸èƒ½ç›´æ¥åŠ ï¼Œéœ€è¦ä¸€äº›math
    
    ![Screen Shot 2022-03-08 at 11.35.34 PM.png](Classifica%2038971/Screen_Shot_2022-03-08_at_11.35.34_PM.png)
    
    ![Screen Shot 2022-03-08 at 11.34.49 PM.png](Classifica%2038971/Screen_Shot_2022-03-08_at_11.34.49_PM.png)